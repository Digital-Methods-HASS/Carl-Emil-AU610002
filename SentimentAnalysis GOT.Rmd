---
title: 'Text mining, sentiment analysis, and visualization'
date: 'created on 22 November 2020 and updated `r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

# Note - Before lab:
# Attach tidytext and textdata packages
get_sentiments(lexicon = "nrc")
# Should be prompted to install lexicon - choose yes!
get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!

```

**Note:** for more text analysis, you can fork & work through Casey O’Hara and Jessica Couture’s eco-data-sci workshop (available here https://github.com/oharac/text_workshop)


## Big picture takeaway

There are serious limitations of sentiment analysis using existing lexicons, and you should **think really hard** about your findings and if a lexicon makes sense for your study. Otherwise, word counts and exploration alone can be useful! 

## Your task

Taking this script as a point of departure, apply sentiment analysis on the Game of Thrones. You will find a pdf in the data folder. What are the most common meaningful words and what emotions do you expect will dominate this volume? Are there any terms that are similarly ambiguous to the 'confidence' above? 

### Credits: 
This tutorial is inspired by Allison Horst's Advanced Statistics and Data Analysis.

# New project GOT
```{r}
got <- here("data", "got.pdf")
got_text <- pdf_text(got)

```


```{r split-lines}
got_df <- data.frame(got_text) %>%
mutate(text_full = str_split(got_text, pattern = '\n')) %>%
unnest(text_full) %>%
mutate(text_full = str_trim(text_full))
```


```{r tokenize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens

```

```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc

```

```



```


```{r stopwords}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)

```

```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)

```

```{r skip-numbers}
got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
         
```

```{r wordcloud-prep}
length(unique(got_no_numeric$word))
got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud

```

````{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
````

```{r afinn}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos

```

```{r bing}
get_sentiments(lexicon = "bing")

```

```{r nrc}
get_sentiments(lexicon = "nrc")

```

```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))

```

```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col()

```

```{r afinn-2}
# What are these '2' words?
got_afinn2 <- got_afinn %>% 
  filter(value == 2)

```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(got_afinn2$word)

# Count & plot them
got_afinn2_n <- got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

```

```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )

```

```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))

```

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)

```

```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()

```

```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("figures","ipcc_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```

```{r nrc-confidence}
conf <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "confidence")

# Yep, check it out:
conf

```

